{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducción de dimensionalidad\n",
    "Este notebook fue construido con recursos de [GitHub](https://github.com/ageron/handson-ml3).\n",
    "\n",
    "En muchas ocasiones, el conjunto de datos que queremos procesar contiene miles o incluso millones de características para cada muestra, lo que puede generar tiempos extremadamente largos de entrenamiento, e incluso hacer más díficil encontrar una buena solución para la tarea que buscamos resolver. Para estos casos, las técnicas de reducción de dimensionalidad permiten eliminar las características menos relevantes, buscando acelerar el entrenamiento sin afectar de una maneara significativa el desempeño de los modelos. Adicionalmente, son una herramienta fundamental para la visualización de datos, que permite realizar análisis visuales de conjuntos de datos y presentar resultados/conclusiones de una manera más efectiva a personas fuera del campo de la ciencia de datos.\n",
    "\n",
    "Otro problema asociado a conjuntos de datos con muchas características (_high-dimensional datasets_) se conoce como _the curse of dimensionality_, y se refiere a que dado el gran \"espacio\" en el que están representados los datos, hay una alta probabilidad de que las muestras de entrenamiento se encuentren muy \"lejos\" unas de otras, generando modelos basados en extrapolar la información disponible, que son más propensos al sobreajuste u _overfitting_.\n",
    "\n",
    "Existen dos formas principales para la reducción de características, la proyección y _manifold learning_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyección vs. _Manifold Learning_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "m = 60\n",
    "X = np.zeros((m, 3))  # initialize 3D dataset\n",
    "np.random.seed(42)\n",
    "angles = (np.random.rand(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\n",
    "X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\n",
    "X += 0.28 * np.random.randn(m, 3)  # add more noise\n",
    "X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n",
    "X += [0.2, 0, 0.2]  # shift a bit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)  # dataset reduced to 2D\n",
    "X3D_inv = pca.inverse_transform(X2D)  # 3D position of the projected samples\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "\n",
    "axes = [-1.4, 1.4, -1.4, 1.4, -1.1, 1.1]\n",
    "x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 10),\n",
    "                     np.linspace(axes[2], axes[3], 10))\n",
    "w1, w2 = np.linalg.solve(Vt[:2, :2], Vt[:2, 2])  # projection plane coefs\n",
    "z = w1 * (x1 - pca.mean_[0]) + w2 * (x2 - pca.mean_[1]) - pca.mean_[2]  # plane\n",
    "X3D_above = X[X[:, 2] >= X3D_inv[:, 2]]  # samples above plane\n",
    "X3D_below = X[X[:, 2] < X3D_inv[:, 2]]  # samples below plane\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# plot samples and projection lines below plane first\n",
    "ax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], \"ro\", alpha=0.3)\n",
    "for i in range(m):\n",
    "    if X[i, 2] < X3D_inv[i, 2]:\n",
    "        ax.plot([X[i][0], X3D_inv[i][0]],\n",
    "                [X[i][1], X3D_inv[i][1]],\n",
    "                [X[i][2], X3D_inv[i][2]], \":\", color=\"#F88\")\n",
    "\n",
    "ax.plot_surface(x1, x2, z, alpha=0.1, color=\"b\")  # projection plane\n",
    "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b+\")  # projected samples\n",
    "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b.\")\n",
    "\n",
    "# now plot projection lines and samples above plane\n",
    "for i in range(m):\n",
    "    if X[i, 2] >= X3D_inv[i, 2]:\n",
    "        ax.plot([X[i][0], X3D_inv[i][0]],\n",
    "                [X[i][1], X3D_inv[i][1]],\n",
    "                [X[i][2], X3D_inv[i][2]], \"r--\")\n",
    "\n",
    "ax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], \"ro\")\n",
    "\n",
    "def set_xyz_axes(ax, axes):\n",
    "    ax.xaxis.set_rotate_label(False)\n",
    "    ax.yaxis.set_rotate_label(False)\n",
    "    ax.zaxis.set_rotate_label(False)\n",
    "    ax.set_xlabel(\"$x_1$\", labelpad=8, rotation=0)\n",
    "    ax.set_ylabel(\"$x_2$\", labelpad=8, rotation=0)\n",
    "    ax.set_zlabel(\"$x_3$\", labelpad=8, rotation=0)\n",
    "    ax.set_xlim(axes[0:2])\n",
    "    ax.set_ylim(axes[2:4])\n",
    "    ax.set_zlim(axes[4:6])\n",
    "\n",
    "set_xyz_axes(ax, axes)\n",
    "ax.set_zticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"b+\")\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"b.\")\n",
    "ax.plot([0], [0], \"bo\")\n",
    "ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True,\n",
    "         head_length=0.1, fc='b', ec='b', linewidth=4)\n",
    "ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True,\n",
    "         head_length=0.1, fc='b', ec='b', linewidth=1)\n",
    "ax.set_xlabel(\"$z_1$\")\n",
    "ax.set_yticks([-0.5, 0, 0.5, 1])\n",
    "ax.set_ylabel(\"$z_2$\", rotation=0)\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "darker_hot = ListedColormap(plt.cm.hot(np.linspace(0, 0.8, 256)))\n",
    "\n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2], c=t, cmap=darker_hot)\n",
    "ax.view_init(10, -70)\n",
    "set_xyz_axes(ax, axes)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea de _Manifold_ se puede expresar de manera simple como una función o un plano $d$-dimensional representado en un espacio $n$-dimensional, donde $d<n$. Por ejemplo, en el caso anterior (conocido como el _Swiss roll_), es un plano 2-dimensional representado en 3 dimensiones como un \"rollo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_swiss[:, 0], X_swiss[:, 1], c=t, cmap=darker_hot)\n",
    "plt.axis(axes[:4])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\", labelpad=10, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(t, X_swiss[:, 1], c=t, cmap=darker_hot)\n",
    "plt.axis([4, 14.8, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "El PCA es la técnica más popular de reducción de dimensionalidad. Es un algoritmo de proyección lineal, en el que primero se encuentra el hiperplano más cercano a los datos y luego proyecta cada muestra sobre él.\n",
    "\n",
    "El hiperplano seleccionado es aquel que preserva máxima variabilidad y ocasiona la menor pérdida de información.\n",
    "\n",
    "Matemáticamente, PCA está basado en una operación de factorización matricial llamada [_Singular Value Decomposition_](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.pi / 5\n",
    "stretch = 5\n",
    "m = 200\n",
    "\n",
    "np.random.seed(3)\n",
    "X_line = np.random.randn(m, 2) / 10\n",
    "X_line = X_line @ np.array([[stretch, 0], [0, 1]])  # stretch\n",
    "X_line = X_line @ [[np.cos(angle), np.sin(angle)],\n",
    "                   [np.sin(angle), np.cos(angle)]]  # rotate\n",
    "\n",
    "u1 = np.array([np.cos(angle), np.sin(angle)])\n",
    "u2 = np.array([np.cos(angle - 2 * np.pi / 6), np.sin(angle - 2 * np.pi / 6)])\n",
    "u3 = np.array([np.cos(angle - np.pi / 2), np.sin(angle - np.pi / 2)])\n",
    "\n",
    "X_proj1 = X_line @ u1.reshape(-1, 1)\n",
    "X_proj2 = X_line @ u2.reshape(-1, 1)\n",
    "X_proj3 = X_line @ u3.reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot2grid((3, 2), (0, 0), rowspan=3)\n",
    "plt.plot([-1.4, 1.4], [-1.4 * u1[1] / u1[0], 1.4 * u1[1] / u1[0]], \"k-\",\n",
    "         linewidth=2)\n",
    "plt.plot([-1.4, 1.4], [-1.4 * u2[1] / u2[0], 1.4 * u2[1] / u2[0]], \"k--\",\n",
    "         linewidth=2)\n",
    "plt.plot([-1.4, 1.4], [-1.4 * u3[1] / u3[0], 1.4 * u3[1] / u3[0]], \"k:\",\n",
    "         linewidth=2)\n",
    "plt.plot(X_line[:, 0], X_line[:, 1], \"ro\", alpha=0.5)\n",
    "plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=4, alpha=0.9,\n",
    "          length_includes_head=True, head_length=0.1, fc=\"b\", ec=\"b\", zorder=10)\n",
    "plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=1, alpha=0.9,\n",
    "          length_includes_head=True, head_length=0.1, fc=\"b\", ec=\"b\", zorder=10)\n",
    "plt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", color=\"blue\")\n",
    "plt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", color=\"blue\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\", rotation=0)\n",
    "plt.axis([-1.4, 1.4, -1.4, 1.4])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot2grid((3, 2), (0, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k-\", linewidth=2)\n",
    "plt.plot(X_proj1[:, 0], np.zeros(m), \"ro\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot2grid((3, 2), (1, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k--\", linewidth=2)\n",
    "plt.plot(X_proj2[:, 0], np.zeros(m), \"ro\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot2grid((3, 2), (2, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k:\", linewidth=2)\n",
    "plt.plot(X_proj3[:, 0], np.zeros(m), \"ro\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.xlabel(\"$z_1$\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "pca.components_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variabilidad explicada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encontrando el número apropiado de componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser='auto')\n",
    "X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
    "X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1  # d equals 154\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
    "             arrowprops=dict(arrowstyle=\"->\"))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Número de componentes principales: {pca.n_components_}\")\n",
    "print(f\"Variabilidad explicada: {pca.explained_variance_ratio_.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for idx, X in enumerate((X_train[::2100], X_recovered[::2100])):\n",
    "    plt.subplot(1, 2, idx + 1)\n",
    "    plt.title([\"Original\", \"Compressed\"][idx])\n",
    "    for row in range(5):\n",
    "        for col in range(5):\n",
    "            plt.imshow(X[row * 5 + col].reshape(28, 28), cmap=\"binary\",\n",
    "                       vmin=0, vmax=255, extent=(row, row + 1, col, col + 1))\n",
    "            plt.axis([0, 5, 0, 5])\n",
    "            plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso _Swiss roll_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_swiss)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Es una técnica dentro del grupo de _Manifold learning_ que reduce la dimensionalidad buscando mantener las muestras similares cerca y las muestras no similares alejadas. Su principal aplicación es la visualización de de grupos (_clusters_) de muestras en un espacio de muchas dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\",\n",
    "            random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(X_swiss)\n",
    "\n",
    "plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=t, cmap=darker_hot)\n",
    "plt.xlabel(\"$z_1$\")\n",
    "plt.ylabel(\"$z_2$\", rotation=0)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample, y_sample = X_train[:5000], y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X_reduced = tsne.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1],\n",
    "            c=y_sample.astype(np.int8), cmap=\"jet\", alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "def plot_digits(X, y, min_distance=0.04, images=None, figsize=(13, 10)):\n",
    "    # Let's scale the input features so that they range from 0 to 1\n",
    "    X_normalized = MinMaxScaler().fit_transform(X)\n",
    "    # Now we create the list of coordinates of the digits plotted so far.\n",
    "    # We pretend that one is already plotted far away at the start, to\n",
    "    # avoid `if` statements in the loop below\n",
    "    neighbors = np.array([[10., 10.]])\n",
    "    # The rest should be self-explanatory\n",
    "    plt.figure(figsize=figsize)\n",
    "    cmap = plt.cm.jet\n",
    "    digits = np.unique(y)\n",
    "    for digit in digits:\n",
    "        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1],\n",
    "                    c=[cmap(float(digit) / 9)], alpha=0.5)\n",
    "    plt.axis(\"off\")\n",
    "    ax = plt.gca()  # get current axes\n",
    "    for index, image_coord in enumerate(X_normalized):\n",
    "        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()\n",
    "        if closest_distance > min_distance:\n",
    "            neighbors = np.r_[neighbors, [image_coord]]\n",
    "            if images is None:\n",
    "                plt.text(image_coord[0], image_coord[1], str(int(y[index])),\n",
    "                         color=cmap(float(y[index]) / 9),\n",
    "                         fontdict={\"weight\": \"bold\", \"size\": 16})\n",
    "            else:\n",
    "                image = images[index].reshape(28, 28)\n",
    "                imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"),\n",
    "                                          image_coord)\n",
    "                ax.add_artist(imagebox)\n",
    "\n",
    "plot_digits(X_reduced, y_sample, images=X_sample, figsize=(35, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
